{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <!-- This div will center all its contents -->\n",
    "  <img src=\"https://scontent.fopo6-1.fna.fbcdn.net/v/t39.30808-6/327345211_708012977623591_5371889953719216000_n.png?_nc_cat=104&ccb=1-7&_nc_sid=5f2048&_nc_eui2=AeGA4Epi5DPgQWGmwJnzDzYwlTHqnE4dPp2VMeqcTh0-ndnVzTPGmZ1C7LYJvEsh0wc&_nc_ohc=oHf3AV_aUB0AX_auBWi&_nc_ht=scontent.fopo6-1.fna&oh=00_AfCTA0yaHCQugeMu_44t-6cLSKGa53d67a0DpQQ-fVTGYg&oe=654F295F\" width=\"570\" height=\"250\" style=\"display: block; margin: auto;\"/> <!-- This will center the image -->\n",
    "  <div><strong style=\"color: #4F5B63;\">Master in Data Science for Social Sciences</strong></div>\n",
    "  <div><strong style=\"color: #4F5B63;\">University of Aveiro</strong></div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-around; align-items: flex-start;\">\n",
    "  <div style=\"width: 100%; padding: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); margin: 10px;\">\n",
    "    <h2><h1 style=\"text-align: center; font-size: 4em; color: #46627F; margin-top: 0; margin-bottom: 0; line-height: 1;\">Topic Modeling using Latent Semantic Analysis</h1>\n",
    "<h1 style=\"text-align: center; color: #B1C0CF; margin-top: 0; margin-bottom: 0; line-height: 1;\"> -Deduce the hidden topic from the document- </h1></h2>\n",
    "      </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Highly recommend going through [this article](https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/) to understand terms like SVD and UMAP. They are leveraged in this article so having a basic understanding of them will help solidify these concepts.*\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. What is a Topic Model?\n",
    "\n",
    "1. When is Topic Modeling used?\n",
    "\n",
    "1. Overview of Latent Semantic Analysis (LSA)\n",
    "\n",
    "1. Implementation of LSA in Python\n",
    "\n",
    "    * Data Reading and Inspection\n",
    "\n",
    "    * Data Preprocessing\n",
    "\n",
    "    * Document-Term Matrix\n",
    "\n",
    "    * Topic Modeling\n",
    "\n",
    "5. Pros and Cons of LSA\n",
    "\n",
    "## What is a topic model?\n",
    "\n",
    "A Topic Model can be defined as an unsupervised technique to discover topics across various text documents. These topics are abstract in nature, i.e., words which are related to each other form a topic. Similarly, there can be multiple topics in an individual document. For the time being, let’s understand a topic model as a black box, as illustrated in the below figure:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/2000/0*o4QTTXD7nHjsYQsK.png)\n",
    "\n",
    "This black box (topic model) forms clusters of similar and related words which are called topics. These topics have a certain distribution in a document, and every topic is defined by the proportion of different words it contains.\n",
    "\n",
    "## When is Topic Modeling used?\n",
    "\n",
    "Recall the example we saw earlier of arranging similar books together. Now suppose you have to perform a similar task with a few digital text documents. You would be able to manually accomplish this, as long as the number of documents is manageable (aka not too many of them). But what happens when there’s an impossible number of these digital text documents?\n",
    "\n",
    "That’s where NLP techniques come to the fore. And for this particular task, topic modeling is the technique we will turn to.\n",
    "\n",
    "![*Source: topix.io/tutorial/tutorial.html*](https://cdn-images-1.medium.com/max/2000/0*E_P2Vkt9ZxrgG1gw.png)**Source: topix.io/tutorial/tutorial.html**\n",
    "\n",
    "Topic modeling helps in exploring large amounts of text data, finding clusters of words, the similarity between documents, and discovering abstract topics. As if these reasons weren’t compelling enough, topic modeling is also used in search engines wherein the search string is matched with the results. Getting interesting, isn’t it? Well, read on then!\n",
    "\n",
    "## Overview of Latent Semantic Analysis (LSA)\n",
    "\n",
    "All languages have their own intricacies and nuances which are quite difficult for a machine to capture (sometimes they’re even misunderstood by us humans!). This can include different words that mean the same thing, and also the words which have the same spelling but different meanings.\n",
    "\n",
    "For example, consider the following two sentences:\n",
    "\n",
    "1. I liked his last **novel** quite a lot.\n",
    "\n",
    "1. We would like to go for a **novel** marketing campaign.\n",
    "\n",
    "In the first sentence, the word ‘novel’ refers to a book, and in the second sentence it means new or fresh.\n",
    "\n",
    "We can easily distinguish between these words because we are able to understand the context behind these words. However, a machine would not be able to capture this concept as it cannot understand the context in which the words have been used. This is where Latent Semantic Analysis (LSA) comes into play as it attempts to leverage the context around the words to capture the hidden concepts, also known as topics.\n",
    "\n",
    "So, simply mapping words to documents won’t really help. What we really need is to figure out the hidden concepts or topics behind the words. LSA is one such technique that can find these hidden topics. Let’s now deep dive into the inner workings of LSA.\n",
    "\n",
    "## Steps involved in the implementation of LSA\n",
    "\n",
    "Let’s say we have **m **number of text documents with **n** number of total unique terms (words). We wish to extract **k** topics from all the text data in the documents. The number of topics, k, has to be specified by the user.\n",
    "\n",
    "* Generate a document-term matrix of shape **m x n **having TF-IDF scores**.**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/2000/0*Gq8AgWwkpQ-pvfaD.png)\n",
    "\n",
    "* Then, we will reduce the dimensions of the above matrix to **k **(no. of desired topics) dimensions, using singular-value decomposition (SVD).\n",
    "\n",
    "* SVD decomposes a matrix into three other matrices. Suppose we want to decompose a matrix A using SVD. It will be decomposed into matrix U, matrix S, and VT (transpose of matrix V).\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/2000/0*ZRAsqV_YN0OOl-iC.png)\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/2000/0*RQ7tGRBA6wiMeGFD.png)\n",
    "\n",
    "Each row in the matrix **Uk (document-term matrix)** is a vector representation of the corresponding document. The length of these vectors is k, which is the number of desired topics. Vector representation for the terms in our data can be found in the matrix **Vk (term-topic matrix)**.\n",
    "\n",
    "* So, SVD gives us vectors for every document and term in our data. The length of each vector would be **k**. We can then use these vectors to find similar words and similar documents using the cosine similarity method.\n",
    "\n",
    "## Implementation of LSA in Python\n",
    "\n",
    "It’s time to power up Python and understand how to implement LSA in a topic modeling problem. Once your Python environment is open, follow the steps I have mentioned below.\n",
    "\n",
    "### Data reading and inspection\n",
    "\n",
    "Let’s load the required libraries before proceeding with anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import numpy as np\n",
    "    import pandas as pd \n",
    "    import matplotlib.pyplot as plt \n",
    "    pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('scopus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      A perspective on computational research support programs in the library: More than 20 years of data from Stanford University Libraries\n",
       "1                                             Handbook of research on artificial intelligence applications in literary works and social media\n",
       "2                       Teaching beginner-level computational social science: interactive open education resources with learnr and shiny apps\n",
       "3                                   Early prediction of student engagement in virtual learning environments using machine learning techniques\n",
       "4                                            mdx: A Cloud Platform for Supporting Data Science and Cross-Disciplinary Research Collaborations\n",
       "                                                                        ...                                                                  \n",
       "352                                                                                                                    One health informatics\n",
       "353                                                              Quantifying the link between art and property prices in urban neighbourhoods\n",
       "354                                                                     Efficiency Improvements in Social Network Communication via MapReduce\n",
       "355                                                                                                Predicting iPhone Sales from iPhone Tweets\n",
       "356                                                                          WebSci 2016 - Proceedings of the 2016 ACM Web Science Conference\n",
       "Name: Title, Length: 357, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document=df['Title']\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "To start with, we will try to clean our text data as much as possible. The idea is to remove the punctuation, numbers, and special characters all in one step using the regex *replace(“[^a-zA-Z#]”, “ ”)*, which will replace everything, except alphabets with space. Then we will remove shorter words because they usually don’t contain useful information. Finally, we will make all the text lowercase to nullify case sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    news_df = pd.DataFrame({'document':document}) \n",
    "\n",
    "    # remove everything except alphabets` \n",
    "    news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") \n",
    "\n",
    "    # remove short words \n",
    "    news_df['clean_doc']=news_df['clean_doc'].apply(lambda x:' '.join([w for w in x.split() if len(w)>3])) \n",
    "\n",
    "    # make all text lowercase \n",
    "    news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s good practice to remove the stop-words from the text data as they are mostly clutter and hardly carry any information. Stop-words are terms like ‘it’, ‘they’, ‘am’, ‘been’, ‘about’, ‘because’, ‘while’, etc.\n",
    "\n",
    "To remove stop-words from the documents, we will have to tokenize the text, i.e., split the string of text into individual tokens or words. We will stitch the tokens back together once we have removed the stop-words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from nltk.corpus import stopwords \n",
    "    stop_words = stopwords.words('english') \n",
    "\n",
    "    # tokenization \n",
    "    tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) \n",
    "\n",
    "    # remove stop-words \n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) \n",
    "\n",
    "    # de-tokenization \n",
    "    detokenized_doc = [] \n",
    "    for i in range(len(news_df)): \n",
    "        t = ' '.join(tokenized_doc[i]) \n",
    "        detokenized_doc.append(t) \n",
    "\n",
    "    news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Term Matrix\n",
    "\n",
    "This is the first step towards topic modeling. We will use sklearn’s *TfidfVectorizer* to create a document-term matrix with 1,000 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, max_df = 0.5, smooth_idf=True) \n",
    "\n",
    "    X = vectorizer.fit_transform(news_df['clean_doc']) \n",
    "\n",
    "    X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have used all the terms to create this matrix but that would need quite a lot of computation time and resources. Hence, we have restricted the number of features to 1,000. If you have the computational power, I suggest trying out all the terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "\n",
    "The next step is to represent each and every term and document as a vector. We will use the document-term matrix and decompose it into multiple matrices. We will use sklearn’s *TruncatedSVD* to perform the task of matrix decomposition.\n",
    "\n",
    "Since the data comes from 20 different newsgroups, let’s try to have 20 topics for our text data. The number of topics can be specified by using the *n_components* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    from sklearn.decomposition import TruncatedSVD \n",
    "\n",
    "    # SVD represent documents and terms in vectors \n",
    "    svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122) \n",
    "\n",
    "    svd_model.fit(X) \n",
    "\n",
    "    len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components of *svd_model* are our topics, and we can access them using *svd_model.components_*. Finally, let’s print a few most important words in each of the 20 topics and see how our model has done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: data science social research sciences computational analytics \n",
      "\n",
      "Topic 1: data science analytics visualization survey approach management \n",
      "\n",
      "Topic 2: learning machine analytics analysis applications based using \n",
      "\n",
      "Topic 3: science learning computational machine artificial intelligence opportunities \n",
      "\n",
      "Topic 4: using based study twitter case networks approach \n",
      "\n",
      "Topic 5: artificial intelligence research media responsible digital using \n",
      "\n",
      "Topic 6: analysis networks network artificial intelligence analytics approaches \n",
      "\n",
      "Topic 7: research digital analysis education methods applications based \n",
      "\n",
      "Topic 8: networks social complex mining learning framework platform \n",
      "\n",
      "Topic 9: twitter networks sciences 19 covid education dataset \n",
      "\n",
      "Topic 10: networks education computational human analytics digital approach \n",
      "\n",
      "Topic 11: twitter 19 covid analytics research dataset computational \n",
      "\n",
      "Topic 12: case education applications study academic performance challenges \n",
      "\n",
      "Topic 13: challenges opportunities computational applications 19 covid based \n",
      "\n",
      "Topic 14: based analytics case internet sciences business interdisciplinary \n",
      "\n",
      "Topic 15: methods media digital communication based making survey \n",
      "\n",
      "Topic 16: cloud using open digital analysis language platform \n",
      "\n",
      "Topic 17: language natural processing network framework workshop international \n",
      "\n",
      "Topic 18: internet patterns challenges things processing predicting human \n",
      "\n",
      "Topic 19: framework education twitter social gender media learning \n",
      "\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key=lambda x: x[1], reverse=True)[:7]\n",
    "    print(\"Topic \" + str(i) + \": \", end=\"\")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0], end=\" \")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic 0: data science social research sciences computational analytics \n",
    "- Topic 1: data science analytics visualization survey approach management \n",
    "- Topic 2: learning machine analytics analysis applications based using \n",
    "- Topic 3: science learning computational machine artificial intelligence opportunities \n",
    "- Topic 4: using based study twitter case networks approach \n",
    "- Topic 5: artificial intelligence research media responsible digital using \n",
    "- Topic 6: analysis networks network artificial intelligence analytics approaches \n",
    "- Topic 7: research digital analysis education methods applications based \n",
    "- Topic 8: networks social complex mining learning framework platform \n",
    "- Topic 9: twitter networks sciences 19 covid education dataset \n",
    "- Topic 10: networks education computational human analytics digital approach \n",
    "- Topic 11: twitter 19 covid analytics research dataset computational \n",
    "- Topic 12: case education applications study academic performance challenges \n",
    "- Topic 13: challenges opportunities computational applications 19 covid based \n",
    "- Topic 14: based analytics case internet sciences business interdisciplinary \n",
    "- Topic 15: methods media digital communication based making survey \n",
    "- Topic 16: cloud using open digital analysis language platform \n",
    "- Topic 17: language natural processing network framework workshop international \n",
    "- Topic 18: internet patterns challenges things processing predicting human \n",
    "- Topic 19: framework education twitter social gender media learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of LSA\n",
    "\n",
    "Latent Semantic Analysis can be very useful as we saw above, but it does have its limitations. It’s important to understand both the sides of LSA so you have an idea of when to leverage it and when to try something else.\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* LSA is fast and easy to implement.\n",
    "\n",
    "* It gives decent results, much better than a plain vector space model.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* Since it is a linear model, it might not do well on datasets with non-linear dependencies.\n",
    "\n",
    "* LSA assumes a Gaussian distribution of the terms in the documents, which may not be true for all problems.\n",
    "\n",
    "* LSA involves SVD, which is computationally intensive and hard to update as new data comes up.\n",
    "\n",
    "\n",
    "References: https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICD_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
