{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <!-- This div will center all its contents -->\n",
    "  <img src=\"https://scontent.fopo6-1.fna.fbcdn.net/v/t39.30808-6/327345211_708012977623591_5371889953719216000_n.png?_nc_cat=104&ccb=1-7&_nc_sid=5f2048&_nc_eui2=AeGA4Epi5DPgQWGmwJnzDzYwlTHqnE4dPp2VMeqcTh0-ndnVzTPGmZ1C7LYJvEsh0wc&_nc_ohc=oHf3AV_aUB0AX_auBWi&_nc_ht=scontent.fopo6-1.fna&oh=00_AfCTA0yaHCQugeMu_44t-6cLSKGa53d67a0DpQQ-fVTGYg&oe=654F295F\" width=\"570\" height=\"250\" style=\"display: block; margin: auto;\"/> <!-- This will center the image -->\n",
    "  <div><strong style=\"color: #4F5B63;\">Master in Data Science for Social Sciences</strong></div>\n",
    "  <div><strong style=\"color: #4F5B63;\">University of Aveiro</strong></div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-around; align-items: flex-start;\">\n",
    "  <div style=\"width: 100%; padding: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); margin: 10px;\">\n",
    "    <h2><h1 style=\"text-align: center; font-size: 4em; color: #46627F; margin-top: 0; margin-bottom: 0; line-height: 1;\">Exemplos NLP</h1>\n",
    "<h1 style=\"text-align: center; color: #B1C0CF; margin-top: 0; margin-bottom: 0; line-height: 1;\"> -PPTs da Aula- </h1></h2>\n",
    "      </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72635eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Downloader.download of <nltk.downloader.Downloader object at 0x000001EC7CCFDCD0>>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa1c8292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá mundo\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"Olá, mundo!\"\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbbdfbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, world! this is an example.\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, World! This is an example.\"\n",
    "text_lower = text.lower()\n",
    "\n",
    "print(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4126cdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gosto café\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "text = \"Eu gosto de café\"\n",
    "text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e81eb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eu', 'gosto', 'de', 'café']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Eu gosto de café\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "967b78ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "stemmer = RSLPStemmer()\n",
    "stemmed_word = stemmer.stem(\"correndo\")\n",
    "print(stemmed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae70b9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_word = lemmatizer.lemmatize(\"changing\", pos=\"v\")\n",
    "print(lemmatized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8ae29b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cão  gato\n",
      "0    1     1\n",
      "1    0     1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([\"gato cão\", \"gato\"])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d5044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "349c792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['cão' 'gato']\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature names:\", vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b5f2fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform([\"gato da rua preto\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01fe6d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cão        da      gato       rua\n",
      "0  0.861037  0.000000  0.508542  0.000000\n",
      "1  0.000000  0.000000  1.000000  0.000000\n",
      "2  0.000000  0.652491  0.385372  0.652491\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([\"gato cão\", \"gato\", \"gato da rua\"])\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c2206ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'), ('cat', 'NN'), ('ran', 'VBD')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo em inglês usando NLTK\n",
    "import nltk\n",
    "\n",
    "text = nltk.word_tokenize(\"The cat ran\")\n",
    "nltk.pos_tag(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa3c2315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usando uma biblioteca como TextBlob (em inglês)\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I love NLP\"\n",
    "blob = TextBlob(text)\n",
    "blob.sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
